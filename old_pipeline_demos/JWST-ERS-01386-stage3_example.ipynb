{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edde56a3-666f-44d8-9856-078694082db9",
   "metadata": {},
   "source": [
    "# Pipeline PSF subtraction of MIRI coronagraphic data\n",
    "\n",
    "Author: Jonathan Aguilar (jaguilar@stsci.edu)\n",
    "Date: July 31, 2023\n",
    "\n",
    "Requirements not part of the Python Standard Library:\n",
    "- numpy https://numpy.org/\n",
    "- matplotlib https://matplotlib.org/\n",
    "- pandas https://pandas.pydata.org/\n",
    "- astropy https://docs.astropy.org/en/stable/\n",
    "- astroquery https://astroquery.readthedocs.io/en/latest/\n",
    "- jwst https://jwst-pipeline.readthedocs.io/en/latest/\n",
    "\n",
    "This two-purpose tutorial will show you how to retrieve data from MAST, and how to run the calibration pipeline's Stage 3 coronagraphy step on MIRI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c406f4-9cc5-422e-9190-560b954a405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from astroquery.mast import Mast\n",
    "from astropy.time import Time\n",
    "\n",
    "from astropy.io import fits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9234a43a-58f5-4092-bca7-14a1fc1c7d88",
   "metadata": {},
   "source": [
    "## Step 1: Retrieve MIRI/CORON data for HIP-65426 from program JWST-ERS-01386\n",
    "\n",
    "The first step in this tutorial is to download the relevant data from MAST, using the MAST API. In the interest of brevity, I have left the code in a state in which it can be simply run, with only a minimal explanation of how it works or how to change parameters. \n",
    "\n",
    "More information on how to use the astroquery interface to MAST can be found here: \n",
    "- The MAST API: https://mast.stsci.edu/api/v0/\n",
    "- The astroquery MAST interface: https://astroquery.readthedocs.io/en/latest/mast/mast.html\n",
    "\n",
    "We're going to download our data strictly using the program ID and observation identifiers, taken from APT.\n",
    "\n",
    "APT files are public, so you can open APT and do `File -> Retrieve from STScI -> Retrieve using Program ID`. Type \"1386\" into the entry field and press `Enter`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a23111-bed5-4de1-bd75-05ae7898d6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prog_id = '1386'\n",
    "observations = [4, 5, 6, 7, 8, 9, 28, 29, 30, 31]\n",
    "observations = [str(i) for i in observations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c02928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_params(parameters):\n",
    "    return [{\"paramName\":p, \"values\":v} for p,v in parameters.items()]\n",
    "\n",
    "def set_mjd_range(min, max):\n",
    "    '''Set time range in MJD given limits expressed as ISO-8601 dates'''\n",
    "    return {\n",
    "        \"min\": Time(min, format='isot').mjd, \n",
    "        \"max\": Time(max, format='isot').mjd\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7454aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the search keyword parameters into a format that can query MAST\n",
    "keywords = {\n",
    "        'program': [prog_id], \n",
    "        'observtn': observations,\n",
    "        'date_obs_mjd': [set_mjd_range('2021-03-01T00:00:00','2023-07-01T00:00:00')]\n",
    "    }\n",
    "params = {\n",
    "    'columns': '*',\n",
    "    'filters': set_params(keywords)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37958b6-3d3c-4b51-9437-3b57b4d1a8af",
   "metadata": {},
   "source": [
    "In this step, we request the metadata of the all data products related to our query. We will then select which subset of the products we want, and retrieve only those.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39de73a-52d0-4cf9-9e42-a68c059c0e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "service = 'Mast.Jwst.Filtered.Miri'\n",
    "t = Mast.service_request(service, params).to_pandas()\n",
    "print(len(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8526732-bc3a-4592-9f43-b28f2003de8d",
   "metadata": {},
   "source": [
    "If `t` has nothing in it (i.e. `len(t) == 0`), your query didn't work and you need to revise your keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee761a32-4ce4-4428-a8d4-a7d1c10489a0",
   "metadata": {},
   "source": [
    "For this example, we only want the Stage 2b, flux-calibrated files. Pull out the names of the products we want to download. \n",
    "\n",
    "Furthermore, for Stage 3 processing, we want to download the ASN pool files and the ASN Table files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c16ebf-05ff-472e-827b-eefa062a352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first 5 lines of the dataframe\n",
    "t.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9b7b73-c79e-4d5f-b752-4f50fc6dfdb6",
   "metadata": {},
   "source": [
    "There are a lot of columns available to help you choose which data to retrieve! We want:\n",
    "- cal/calints data: `productLevel == '2b'`\n",
    "    - These are stored under the column `filename`\n",
    "- Association files for Stage 3: `productLevel == '3'`\n",
    "    - These are stored under the columns `asntable` and `asnpool`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5f4c96-d681-41eb-a06c-02de40246c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cal files\n",
    "products = list(t.query(\"productLevel == '2b'\")['filename']) \n",
    "# asn files\n",
    "products = products + list([i for i in t.query(\"productLevel == '3'\")['asntable'] if 'coron3' in i])\n",
    "# pool files\n",
    "products = products + list(t.query(\"productLevel == '3'\")['asnpool'])\n",
    "print(f\"We are going to retrieve the following {len(products)} files:\\n\")\n",
    "for p in sorted(products):\n",
    "    print(\"\\t- \"+p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeecd0b8",
   "metadata": {},
   "source": [
    "The following cell displays the constructed parameter object to illustrate the syntax for the query, which is described formally [here](https://mast.stsci.edu/api/v0/_services.html#MastScienceInstrumentKeywordsNircam). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9491f5c9",
   "metadata": {},
   "source": [
    "The full selection of keywords upon which to build search criteria is described in the [Field Descriptions for JWST Instrument Keywords](https://mast.stsci.edu/api/v0/_jwst_inst_keywd.html). Note that [astroquery.mast](https://astroquery.readthedocs.io/en/latest/mast/mast.html) parameter names do not always match the FITS keyword names. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d584c72a",
   "metadata": {},
   "source": [
    "## SI Keyword Search\n",
    "<a id=\"KW Search\"></a>\n",
    "\n",
    "This type of query is a little more primitive in [astroquery.mast](https://astroquery.readthedocs.io/en/latest/mast/mast.html) than that for the `Observation` class. Begin by specifying the web service for the query, which for this case is the [SI keyword search for Miri](https://mast.stsci.edu/api/v0/_services.html#MastScienceInstrumentKeywordsMiri). Then execute the query with arguments for the service and the search parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9568f34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mast_url='https://mast.stsci.edu/api/v0.1/Download/file'\n",
    "uri_prefix = 'mast:JWST/product/'\n",
    "# tokens are only needed for proprietary data. Program 1386 is publicly available. Uncomment the line below if necessary.\n",
    "# token = 'paste-token-here'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c435e2fd-22e3-4956-96bd-38a81f92f7fb",
   "metadata": {},
   "source": [
    "Output into a storage directory, `data-bkp`. We will then copy the files into the directory containing this notebook, so that they can be read by the association file downloaded from MAST. \n",
    "\n",
    "With the original files kept in a separate folder, we can easily restore the working files to their original state if they get modified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea71c3f-066b-4d9d-8b1b-eaacacf67b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"./data-bkp\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "elif not os.access(out_dir, os.W_OK):\n",
    "    raise ValueError(f\"Output directory {out_dir} is not writable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fded7edf-c877-4c05-8a84-3686063bd6ee",
   "metadata": {},
   "source": [
    "Finally, download the data products. There is a condition to skip downloading a file if it already exists. If you want to force a download, either delete the existing files or remove the condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b682822-24bd-430f-b3df-200e28846ca9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "block_dataze = 1024000\n",
    "for i, p in enumerate(products):\n",
    "    r = requests.get(mast_url, params=dict(uri=uri_prefix+p), stream=True\n",
    "                # include the following argument if authentication is needed\n",
    "                #, headers=dict(Authorization=f\"token {token}\")\n",
    "                    )\n",
    "    # check for mal-formed HTTP requests\n",
    "    try:\n",
    "        r.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"Download failed for {p}\")\n",
    "        continue\n",
    "    outfile = os.path.join(out_dir, p)\n",
    "\n",
    "    if os.path.isfile(outfile):\n",
    "        # file already exists, skip downloading it.\n",
    "        print(f\"{outfile} already exists, skipping.\")\n",
    "        continue\n",
    "    else:\n",
    "        with open(outfile, 'wb') as fd:\n",
    "            for data in r.iter_content(chunk_size=block_size):\n",
    "                fd.write(data)\n",
    "    \n",
    "    if not os.path.isfile(outfile):\n",
    "        print(\"ERROR: \" + outfile + \" failed to download.\")\n",
    "    else:\n",
    "        print(f\"COMPLETE: \", outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcdb6a5-5087-48b4-a403-e0e86d18d4ad",
   "metadata": {},
   "source": [
    "The data have been placed in a subfolder of this directory, called `data-bkp`. For `calwebb_coron3`, the data need to be in the same folder as the notebook. \n",
    "\n",
    "I suggest you copy the data to the working folder, so that the original versions can be restored if the images are modified (intentionally or accidentally), without having to download them again from MAST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b6842d-3c9a-40db-ac8c-f918073a2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this works on Unix-like systems like macOS and Linux. Not sure about Windows.\n",
    "!cp ./data-bkp/* ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfeae9e-afaf-499e-ad29-5888fc835385",
   "metadata": {},
   "source": [
    "## Part 2: PSF subtraction with `calwebb_coron3`\n",
    "\n",
    "This divides the PSF subtraction process up into the following steps:\n",
    "- data organizing\n",
    "- data inspection\n",
    "- background subtraction (if necessary)\n",
    "- running pipeline Stage 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36d5204-4529-4868-a3e1-e55e95116250",
   "metadata": {},
   "source": [
    "### Split up the files into the different components\n",
    "\n",
    "We're going to read all the `PRI` headers and put them all into a Pandas DataFrame, which will allow us to make use of DataFrame filtering tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c788cf24-205b-4cea-8130-f6dbc4b4c97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = sorted(Path(out_dir).glob(\"*fits\"))\n",
    "len(data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b373d0-b75b-4d22-b6c3-814111a044fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "asn_files = sorted(Path(out_dir).glob(\"*json\"))\n",
    "len(asn_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97099e8d-a0f1-4560-bd87-a70eb297c29a",
   "metadata": {},
   "source": [
    "I find that it can be helpful to take the all the PRI headers and put them into a pandas DataFrame. The columns are the header keywords, and each row contains the keyword values for one exposure. This makes it easy to filter the dataset by keyword and figure out which files you want for different steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012cd8c5-9768-4dfd-a0be-892b032e35d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_files_by_hdr(files, extnum=0):\n",
    "    \"\"\"\n",
    "    Put all the header keywords into a dataframe for sorting and filtering.\n",
    "    Give it a list of the files, and the extension to take the headers from\n",
    "    \"\"\"\n",
    "    hdrs = []\n",
    "    for f in data_files:\n",
    "        hdr = fits.getheader(str(f), 0)\n",
    "        hdr = pd.Series(hdr)\n",
    "        # drop duplicated index entries, usually this is \"''\"\" and \"COMMENT\"\n",
    "        drop_index = hdr.index[hdr.index.duplicated()]\n",
    "        hdr.drop(index=drop_index, inplace=True)\n",
    "        # also explcitly drop all instances of \"''\" and 'COMMENT'\n",
    "        for label in ['','COMMENT']:\n",
    "            try:\n",
    "                hdr.drop(labels=label)\n",
    "            except KeyError:\n",
    "                # probably this means there are no entries with this index label\n",
    "                pass\n",
    "        hdrs.append(hdr)\n",
    "    hdr_df = pd.concat(hdrs, axis=1).T\n",
    "    return hdr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cedea2c-e2f8-42cb-bd6a-4b4a290b8bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr_df = organize_files_by_hdr(data_files, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6438fd-4349-4d52-b2c4-c8a6b93801ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d59a1e6-3d0e-46da-9e14-45d622b9fbf6",
   "metadata": {},
   "source": [
    "### Prepare the exposures for PSF subtraction\n",
    "\n",
    "The MIRI coronagraphic observations of HIP-65426 are structured as follows (see APT file):\n",
    "\n",
    "- 1140 Coronagraph\n",
    "    - Obs 4: Science target, roll 2\n",
    "        - 2 target acquisition exposures\n",
    "        - 1 occulted exposure\n",
    "    - Obs 5: Science target, roll 1\n",
    "        - 2 target acquisition exposures\n",
    "        - 1 occulted exposure\n",
    "    - Obs 6: Reference target\n",
    "        - 2 target acquisition exposures\n",
    "        - 9 occulted exposure (dithered)\n",
    "    - Obs 28: Science target background exposure\n",
    "        - 1 background exposure\n",
    "    - Obs 29: Science target background exposure\n",
    "        - 1 background exposure\n",
    "- 1550 Coronagraph\n",
    "    - Obs 7: Science target, roll 1\n",
    "        - 2 target acquisition exposures\n",
    "        - 1 occulted exposure\n",
    "    - Obs 8: Science target, roll 2\n",
    "        - 2 target acquisition exposures\n",
    "        - 1 occulted exposure\n",
    "    - Obs 9: Reference target\n",
    "        - 2 target acquisition exposures\n",
    "        - 9 occulted exposure (dithered)\n",
    "    - Obs 30: Science target background exposure\n",
    "        - 1 background exposure\n",
    "    - Obs 31: Science target background exposure\n",
    "        - 1 background exposure\n",
    "\n",
    "Note: Later coronagraphic programs used a 2-point dither pattern for the background, but this was not yet in place when 01386 executed. These programs will have 2 background exposures for each background observation, not 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9462fc-4cd2-4ca2-956e-6f254da0e776",
   "metadata": {},
   "source": [
    "Now that we have the files, let's split them up for their different roles in PSF subtraction. Header keywords can be helpful, as well as the APT file.\n",
    "\n",
    "In this example, we're going to use the observation numbers from the APT file to match exposures, but here are some helpful keywords anyway:\n",
    "\n",
    "- `BGKDTARG` -> True if a background observation, else False\n",
    "- `EXP_TYPE` -> MIR_TACQ for TA images, otherwise MIR_4QPM and MIR_LYOT for occulted exposures\n",
    "- `IS_PSF`   -> True if the star is a PSF reference target, False if it's a science target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f688978f-2a2a-46c8-b152-fba685ae7f70",
   "metadata": {},
   "source": [
    "## 1140 data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59e28ca-f726-4d97-a977-919d432ea2de",
   "metadata": {},
   "source": [
    "First, let's take a quick look at all the 1140 exposures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bedced2-a852-45d3-bd71-afff2d43f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdrs_1140 = hdr_df.query(\"SUBARRAY == 'MASK1140'\")\n",
    "nfiles = len(hdrs_1140)\n",
    "ncols = int(np.ceil(np.sqrt(nfiles)))\n",
    "nrows = int(np.ceil(nfiles/ncols))\n",
    "fig, axes = plt.subplots(figsize=(12,12), ncols=ncols, nrows=nrows)\n",
    "for irow, ax in zip(hdrs_1140.iterrows(), axes.ravel()):\n",
    "    i, row = irow\n",
    "    img = fits.getdata(Path(out_dir) / row['FILENAME'], 1)\n",
    "    # collapse cubes to 2-D for plotting\n",
    "    while np.ndim(img) > 2:\n",
    "        img = img.mean(axis=0)\n",
    "    norm = np.nanquantile(img, [0.05, 0.95])\n",
    "    # we will plot images with the convention that the middle of the lower left pixel is (1, 1)\n",
    "    y, x = [np.linspace(0.5, i+0.5, i+1) for i in img.shape]\n",
    "    ax.pcolor(x, y, img, vmin=norm[0], vmax=norm[1])\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(f\"Obs {row['OBSERVTN']} Dith {row['PATT_NUM']}\")\n",
    "\n",
    "# turn off extra plots\n",
    "for ax in axes.flat[nfiles:]:\n",
    "    ax.set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e3206a-7908-4b47-a03b-283dde4fa02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the rows for the science target. We include the EXP_TYPE keyword to filter out the TA exposurs\n",
    "sci_targ_files = hdr_df.query(f\"OBSERVTN in {['004', '005']} and EXP_TYPE == 'MIR_4QPM'\") \n",
    "# Background observations don't perform TA\n",
    "sci_bgnd_file = hdr_df.query(\"OBSERVTN == '028'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb26fb1-0acb-4ec3-8e0e-1bcc90630b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_bgnd_file = hdr_df.query(\"OBSERVTN == '029'\")\n",
    "ref_targ_files = hdr_df.query(f\"OBSERVTN == '006' and EXP_TYPE == 'MIR_4QPM'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69714e87-e718-4beb-a27d-047ad4c6fc3b",
   "metadata": {},
   "source": [
    "It looks like the backgrounds were subtracted correctly by the pipeline off of the science exposures, but not from the reference exposures. This happens sometimes, so we'll do it ourselves.\n",
    "\n",
    "Let's subtract the background from the reference target exposures by switching `if False` to `if True` (and then set it back again, you only want to run this once).\n",
    "We'll overwrite the file with the new background-subtracted image.\n",
    "\n",
    "Once you're finished with this step, you can regenerate the above plots to see the status of the exposures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec30ced1-9784-49f7-8c2d-5ac8a8462237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Science exposyre background subtraction\n",
    "# We only want to run background subtraction once. \n",
    "# After you run it, set this to False. You can set it back later.\n",
    "if False:\n",
    "    bgnd_img = fits.getdata((Path(out_dir) / sci_bgnd_file['FILENAME'].iloc[0]), 1)\n",
    "    for f in sci_targ_files['FILENAME']:\n",
    "        with fits.open(Path(out_dir) / f) as hdulist:\n",
    "            #hdulist.writeto(str(Path(out_dir) / f)+\"-bkp\")\n",
    "            sci_img = hdulist[1].data\n",
    "            sci_sub = sci_img - bgnd_img\n",
    "            hdulist[1].data = sci_sub\n",
    "            hdulist.writeto(str(Path(out_dir) / f), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e46742-ae71-4377-8cbb-9511b98cb23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference target background subtraction\n",
    "if False:\n",
    "    bgnd_img = fits.getdata((Path(out_dir) / ref_bgnd_file['FILENAME'].iloc[0]), 1)\n",
    "    for f in ref_targ_files['FILENAME']:\n",
    "        with fits.open(Path(out_dir) / f) as hdulist:\n",
    "            img = hdulist[1].data\n",
    "            sub = img - bgnd_img\n",
    "            hdulist[1].data = sub\n",
    "            hdulist.writeto(str(Path(out_dir) / f), overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cfb8e0-1b9b-4d80-93c7-782329d92673",
   "metadata": {},
   "source": [
    "Now you have the glowstick-subtracted science and reference files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdba4d2b-a209-4169-910c-dde535a5a7cc",
   "metadata": {},
   "source": [
    "Now we're ready to run the Stage 3 pipeline. Let's import it, and set the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5a1fb0-5d72-40a4-9555-0c3181b5997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jwst.pipeline import Coron3Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2014b1d8-1524-459a-aa63-48c74ae1b20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Coron3Pipeline().get_pars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96cf081-7dd3-4b25-b972-8efb58842b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "params['save_results'] = True\n",
    "params['output_dir'] = './pipeline_output/1140'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e2d335-3616-446d-8e58-ff82cd892abd",
   "metadata": {},
   "source": [
    "Now you have to select the association file. You want to look for the one pertaining to the 1140 data, and if there are multiple, you want the one with the most recent version of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa7cba-081d-4c8a-8c77-701b7c07ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want the ASN file for the 1140 data. \n",
    "asn_filename = \"./jw01386-c1021_20230717t003100_coron3_00001_asn.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e107a84f-a426-49f0-b63b-30c2d4250e9b",
   "metadata": {},
   "source": [
    "Finally, run the pipeline step. Go get something to drink, because this might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9888828c-e0c4-4e49-8bed-6d2c9ba0675f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cor3 = Coron3Pipeline().call(asn_filename, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452ac506-3545-4420-ac40-240d2f288fdb",
   "metadata": {},
   "source": [
    "## 1550 data\n",
    "\n",
    "We're going to repeat the same steps as with the 1140 data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40044de3-e963-407a-994f-bbc029ac7a54",
   "metadata": {},
   "source": [
    "First off, take a look at the different data products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c256c2b0-174e-417d-a587-10bf45469a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdrs_1550 = hdr_df.query(\"SUBARRAY == 'MASK1550'\")\n",
    "nfiles = len(hdrs_1550)\n",
    "ncols = int(np.ceil(np.sqrt(nfiles)))\n",
    "nrows = int(np.ceil(nfiles/ncols))\n",
    "fig, axes = plt.subplots(figsize=(12,12), ncols=ncols, nrows=nrows)\n",
    "for irow, ax in zip(hdrs_1550.iterrows(), axes.ravel()):\n",
    "    i, row = irow\n",
    "    img = fits.getdata(Path(out_dir) / row['FILENAME'], 1)\n",
    "    # collapse cubes to 2-D for plotting\n",
    "    while np.ndim(img) > 2:\n",
    "        img = img.mean(axis=0)\n",
    "    norm = np.nanquantile(img, [0.05, 0.95])\n",
    "    y, x = [np.linspace(0.5, i+0.5, i+1) for i in img.shape]\n",
    "    ax.pcolor(x, y, img, vmin=norm[0], vmax=norm[1])\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(f\"Obs {row['OBSERVTN']} Dith {row['PATT_NUM']}\")\n",
    "\n",
    "# turn off extra plots\n",
    "for ax in axes.flat[nfiles:]:\n",
    "    ax.set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803065b0-d116-4c1a-9545-4d61764afa2f",
   "metadata": {},
   "source": [
    "This time, it looks like only Obs 9 had the background subtracted automatically by the pipeline. We need to remove the glowsticks for Obs 8, as well as for Obs 7 (the reference star)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d76084-07b8-4dcb-a0cd-1402c7d044a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the rows for the science target. We include the EXP_TYPE keyword to filter out the TA exposures\n",
    "sci_targ_files = hdr_df.query(f\"OBSERVTN in {['008', '009']} and EXP_TYPE == 'MIR_4QPM'\") \n",
    "# Background observations don't perform TA\n",
    "sci_bgnd_file = hdr_df.query(\"OBSERVTN == '030'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88c80d0-074b-48cf-9f4d-92065cf80c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_targ_files = hdr_df.query(f\"OBSERVTN == '007' and EXP_TYPE == 'MIR_4QPM'\")\n",
    "ref_bgnd_file = hdr_df.query(\"OBSERVTN == '031'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8be04a3-0e47-4b25-b6d3-4464ec65401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only want to run background subtraction once. \n",
    "# After you run it, set this to False. You can set it back later.\n",
    "if False:\n",
    "    bgnd_img = fits.getdata((Path(out_dir) / sci_bgnd_file['FILENAME'].iloc[0]), 1)\n",
    "    for f in sci_targ_files[sci_targ_files['OBSERVTN'] == '008']['FILENAME']:\n",
    "        with fits.open(Path(out_dir) / f) as hdulist:\n",
    "            sci_img = hdulist[1].data\n",
    "            sci_sub = sci_img - bgnd_img\n",
    "            hdulist[1].data = sci_sub\n",
    "            hdulist.writeto(str(Path(out_dir) / f), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9fcd68-4065-4c2d-b1ec-9540b9dcddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    bgnd_img = fits.getdata((Path(out_dir) / ref_bgnd_file['FILENAME'].iloc[0]), 1)\n",
    "    for f in ref_targ_files['FILENAME']:\n",
    "        with fits.open(Path(out_dir) / f) as hdulist:\n",
    "            img = hdulist[1].data\n",
    "            sub = img - bgnd_img\n",
    "            hdulist[1].data = sub\n",
    "            hdulist.writeto(str(Path(out_dir) / f), overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09b1bf5-e910-4fc6-a323-3473664a46a6",
   "metadata": {},
   "source": [
    "Now that you have the glowstick-subtracted science and reference files, they are ready for PSF subtraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36131fcb-4a5e-4d45-b698-2f66e7e0ae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Coron3Pipeline().get_pars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a389d1-2a32-4d3e-a83d-75e71cf0fb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "params['save_results'] = True\n",
    "params['output_dir'] = './pipeline_output/1550/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e228b5-d777-4c8b-822f-db6d7ada5d23",
   "metadata": {},
   "source": [
    "Now you have to select the association file. You want to look for the one pertaining to the 1140 data, and if there are multiple, you want the one with the most recent version of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9923bebe-ae89-49bb-8ecc-8f09c9ab99fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the association file you choose lists the correct files, with the correct descriptions\n",
    "asn_filename = \"./jw01386-c1022_20230717t003100_coron3_00001_asn.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a001cf-f27a-4c8a-8e88-a7ca5b18ec4d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cor3 = Coron3Pipeline().call(asn_filename, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f363ad87-ac7e-4b80-8baf-c7a5a72596e6",
   "metadata": {},
   "source": [
    "## Part 3: Examining the output\n",
    "\n",
    "`calwebb_coron3` has several data products besides `i2d` and `crfints` files that are common to other pipeline Stage 3 classes. The descriptions are taken from https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_coron3.html#calwebb-coron3.\n",
    "- `_crfints.fits`: In these files, the DQ array has been updated to flag pixels detected as outliers.\n",
    "- `_median.fits`: This is used by the cosmic ray flagging step.\n",
    "- `_i2d.fits`: The resample step the PSF-subtracted products, usually separated by a roll angle, into a single 2-D image.\n",
    "- `_psfstack.fits`: The data from each input PSF reference exposure are concatenated into a single combined 3D stack by the `stack_refs` step\n",
    "- `_psfalign.fits`: For each science target exposure, all of the reference PSF images in the `_psfstack` product are aligned to each science target integration and saved to a 4D `_psfalign` product by the `align_refs` step\n",
    "- `_psfsub.fits`: For each science target exposure, the klip step applies PSF fitting and subtraction for each integration, resulting in a 3D stack of PSF-subtracted images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b8f6c0-7179-4de9-82e5-330977fbcd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_paths = {'1140': \"./pipeline_output/1140/\",\n",
    "                '1550': \"./pipeline_output/1550/\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e91fa1-7c3e-4a84-8dd4-514afe9d416a",
   "metadata": {},
   "source": [
    "Let's split the files up into the different types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc85349-35fb-4ce9-bdc1-3dbcf9b8f3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmic_files = {coro: sorted(Path(path).glob(\"*crfints*\")) for coro, path in output_paths.items()}\n",
    "median_files = {coro: sorted(Path(path).glob(\"*median*\")) for coro, path in output_paths.items()}\n",
    "resamp_files = {coro: sorted(Path(path).glob(\"*i2d*\")) for coro, path in output_paths.items()}\n",
    "psfstack_files = {coro: sorted(Path(path).glob(\"*psfstack*\")) for coro, path in output_paths.items()}\n",
    "psfalign_files = {coro: sorted(Path(path).glob(\"*psfalign*\")) for coro, path in output_paths.items()}\n",
    "psfsub_files = {coro: sorted(Path(path).glob(\"*psfsub*\")) for coro, path in output_paths.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04131f9e-b382-4e4c-9e8b-931ba281929d",
   "metadata": {},
   "source": [
    "Here are the final derotated-and-combined PSF subtracted images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1775d626-644f-483b-86d9-2815a12ccb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(16, 8))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.set_title(\"1140\")\n",
    "img = fits.getdata(resamp_files[\"1140\"][0], 1)\n",
    "while np.ndim(img) > 2:\n",
    "    img = img.mean(axis=0)\n",
    "norm = np.nanquantile(img, [0.05, 0.95])\n",
    "y, x = [np.linspace(0.5, i+0.5, i+1) for i in img.shape]\n",
    "ax.pcolor(x, y, img, vmin=norm[0], vmax=norm[1])\n",
    "\n",
    "ax = axes[1]\n",
    "ax.set_title(\"1550\")\n",
    "img = fits.getdata(resamp_files[\"1550\"][0], 1)\n",
    "while np.ndim(img) > 2:\n",
    "    img = img.mean(axis=0)\n",
    "norm = np.nanquantile(img, [0.05, 0.95])\n",
    "y, x = [np.linspace(0.5, i+0.5, i+1) for i in img.shape]\n",
    "ax.pcolor(x, y, img, vmin=norm[0], vmax=norm[1])\n",
    "\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc7226b-1288-43a7-9f4c-0629eefb4f35",
   "metadata": {},
   "source": [
    "The remaining images are data cubes and best viewed in a FITS viewer like DS9 or ginga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a5041-8919-4ade-8d97-5db963158d64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jwst_inspector]",
   "language": "python",
   "name": "conda-env-jwst_inspector-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
