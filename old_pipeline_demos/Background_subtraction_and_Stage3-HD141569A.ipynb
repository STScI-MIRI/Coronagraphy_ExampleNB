{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81ab30bb-72aa-40e9-a420-fed85707ed54",
   "metadata": {},
   "source": [
    "# Background Subtraction and Stage 3 Processing\n",
    "\n",
    "- Author: J. Aguilar (jaguilar@stsci.edu)\n",
    "- Date: Aug 9, 2022\n",
    "\n",
    "\n",
    "The presence of glow sticks in the MIRI coronagraph means that, for the time being, users must acquire background observations and subtract them off their science observations to perform accurate PSF subtraction. This notebook will guide users through the process of:\n",
    "- Identifying which files are background observations\n",
    "- Combining and subtracting the backgrounds from the science observations\n",
    "- Running the new background-subtracted images through Stage 3 of the JWST calibration pipeline by modifying the ASN file\n",
    "\n",
    "This notebook is not intended to recommend the best background subtraction or PSF subtraction strategies, it is simply intended to help users identify which files are involved in which steps.\n",
    "\n",
    "Please note that there are a few places where the user will need to either modify file paths, or make sure those paths exist.\n",
    "\n",
    "## Data\n",
    "The ERS data used in this tutorial are available at on MAST (mast.stsci.edu). The following link provides a shortcut: https://mast.stsci.edu/portal/Mashup/Clients/Mast/Portal.html?searchQuery=%7B%22service%22%3A%22CAOMBYPROPID%22%2C%22inputText%22%3A%5B%7B%22paramName%22%3A%22proposal_id%22%2C%22niceName%22%3A%22proposal_id%22%2C%22values%22%3A%5B%221386%22%5D%2C%22valString%22%3A%221386%22%2C%22displayString%22%3A%221386%22%2C%22isDate%22%3Afalse%2C%22facetType%22%3A%22discrete%22%7D%5D%2C%22paramsService%22%3A%22Mast.Caom.Filtered%22%2C%22title%22%3A%22Proposal%20ID%20Results%22%2C%22columns%22%3A%22*%22%2C%22caomVersion%22%3Anull%7D\n",
    "\n",
    "Here are the files on Box: https://stsci.box.com/s/2pw35d61uz6dr4xfrv9znoty1hdy9sfo\n",
    "\n",
    "## Non-standard library requirements\n",
    "- `jwst` https://jwst-pipeline.readthedocs.io/\n",
    "- `astropy` https://docs.astropy.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb7856a-2200-4e03-93be-92bf8f6fe8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jwst\n",
    "print(\"Using pipeline version:\", jwst.__version__)\n",
    "from jwst.pipeline import Coron3Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb145b-ed7d-4c4d-844f-a61823b3eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from astropy.io import fits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82083634-8769-4e0e-937b-7b462ff61ad2",
   "metadata": {},
   "source": [
    "# Load the observations\n",
    "\n",
    "We're going to assume that there are `calints.fits` (aka Level 2b) files available from MAST, but the backgrounds didn't get subtracted properly. If you only have `rateints.fits` (Level 2a) or `uncal.fits` (Level 1) files, refer to the pipeline documentation for how to process them up to Level 2b.\n",
    "\n",
    "Set the variable `data_folder` to wherever you downloaded the MAST data. I assume all the `.fits` files are in a single folder. If you have it set up differently, the important thing is that the variable `data_files` is a list of paths to the `calints.fits` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22536d4b-48d0-4de4-bd4a-90f71b30c2ca",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_folder = Path(\"/Users/jaguilar/Projects/jwst_science_programs/mast_data/01386-2/01386/\")\n",
    "data_files = sorted(data_folder.glob(\"j*calints.fits\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8362017b-2812-469d-979b-7e6801cbae0d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# enforce that all the members of data_files are pathlib.Paths\n",
    "for i, f in enumerate(data_files):\n",
    "    data_files[i] = Path(f)\n",
    "for i in data_files:\n",
    "    print(str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308397ed-6ec5-419b-81e7-53e3018e1cd8",
   "metadata": {},
   "source": [
    "## Organize the science and background observations so you can subtract the appropriate ones from each other.\n",
    "\n",
    "We're going to use the observation number as the key for associating the background observations with their corresponding science observations.\n",
    "\n",
    "We'll store everything in dictionaries and use the file names and observation numbers to look things up.\n",
    "\n",
    "Files that have the same observation number are different dithers from the same observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2a7647-1a56-42d3-9152-507ec711d9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index them by the observation number\n",
    "obsnum_filenames = {f.name: Path(f).name[7:10] for f in data_files}\n",
    "obsnum_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc0b957-a936-4a57-b65f-e86bbe007b36",
   "metadata": {},
   "source": [
    "# Match the background files\n",
    "\n",
    "Let's connect the target observations (science and reference) with their corresponding background observations.\n",
    "\n",
    "Use the observation number from the APT file.\n",
    "\n",
    "Format is list of tuples with pairs in the format (target, background)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7be0648-e21d-47cc-b2cf-2cca15ff0df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the target observations\n",
    "targ_obsnums = ['019', '020', '021', '022', '023', '024', '025', '026', '027']\n",
    "# the background observations\n",
    "bgnd_obsnums = ['032', '033', '034', '035', '036', '037']\n",
    "# match them to each other\n",
    "bgnd_obsnum_mapper = [\n",
    "    ('019', '032'), # F1065C Sci Roll 1\n",
    "    ('020', '032'), # F1065C Sci Roll 2\n",
    "    ('021', '033'), # F1065C Ref\n",
    "    ('022', '034'), # F1140C Sci Roll 1\n",
    "    ('023', '034'), # F1140C Sci Roll 2\n",
    "    ('024', '035'), # F1140C Ref\n",
    "    ('025', '035'), # F1550C Sci Roll 1\n",
    "    ('026', '036'), # F1550C Sci Roll 2\n",
    "    ('027', '037'), # F1550C Ref\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca69c925-70b0-496c-812e-0fe6631d044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the background exposure filenames - there's one or more for each background observation number\n",
    "bgnd_obsnums = [i[1] for i in bgnd_obsnum_mapper]\n",
    "bgnd_files  = {obsnum: [] for obsnum in bgnd_obsnums}\n",
    "for f, num in obsnum_filenames.items():\n",
    "    if num in bgnd_obsnums:\n",
    "        bgnd_files[num].append(f)\n",
    "    else:\n",
    "        pass\n",
    "print(\"Background exposure files:\")\n",
    "for k, v in bgnd_files.items():\n",
    "    print(f\"Obs {k}\\t\" + \"\\n\\t\".join(i for i in v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86654ba-73ad-4676-8323-76cccb007ee0",
   "metadata": {},
   "source": [
    "## Create the composite background images\n",
    "\n",
    "If science exposures have more than one associated background exposure, combine the backgrounds using min or median before subtracting. We also choose to collapse the background image into 2-D before subtraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352e5c1a-4966-4ba2-b9a0-4ae1248fcabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull the background images from the files, and combine them if there's more than one\n",
    "bgnd_imgs = {}\n",
    "for num, files in bgnd_files.items():\n",
    "    bgnd_img = np.stack([fits.getdata(data_folder / f, 1) for f in files])\n",
    "    \n",
    "    # define the function you will use to combine the images. Here we use a simple one, but you can write your own.\n",
    "    func = np.nanmedian if len(files) > 2 else np.nanmin\n",
    "    # apply the function to the background exposures\n",
    "    bgnd_img = func(bgnd_img, axis=0)\n",
    "    \n",
    "    # flatten the background image until it's 2-D\n",
    "    while np.ndim(bgnd_img) > 2:\n",
    "        bgnd_img = np.nanmean(bgnd_img, axis=0)\n",
    "    \n",
    "    bgnd_imgs[num] = bgnd_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7471e2-9be0-40b8-a1fe-4951597fb5ee",
   "metadata": {},
   "source": [
    "## Match target and background *images*\n",
    "\n",
    "Match the target images with their corresponding background images using a dictionary containing each matched pair.\n",
    "\n",
    "The target observation has key 'targ', and its background observation has key 'bgnd'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0697dcb-8c30-4827-9442-a1eec9b44c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "bgnd_img_mapper = {}\n",
    "for f, targ_obsnum in obsnum_filenames.items():\n",
    "    if targ_obsnum not in targ_obsnums:\n",
    "        continue\n",
    "    else:\n",
    "        targ_img = fits.getdata(data_folder / f, 1)\n",
    "        bgnd_obsnum = dict(bgnd_obsnum_mapper)[targ_obsnum]\n",
    "        bgnd_img = bgnd_imgs[bgnd_obsnum]\n",
    "        bgnd_img_mapper[f] = {'targ': targ_img, 'bgnd': bgnd_img}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67360824-7fda-4f7e-9803-3e7a3886130f",
   "metadata": {},
   "source": [
    "# Subtract the backgrounds\n",
    "\n",
    "Now that you have matched each science file with its background (and combined backgrounds if necessary), you can subtract them from each other. \n",
    "\n",
    "We're going to store the results in a dict where the index is the original target file's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af614c29-70f7-48a6-ae47-4876e376a3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bgnd_sub_imgs = {} # this will be indexed by the original name of the target file\n",
    "for targ_file, pair in bgnd_img_mapper.items():\n",
    "    targ_img = pair['targ']\n",
    "    bgnd_img = pair['bgnd']\n",
    "    # Subtract the background off. Since the bgnd is 2-D, the array shapes should automatically broadcast\n",
    "    img = targ_img - bgnd_img\n",
    "    bgnd_sub_imgs[targ_file] = img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63cfbf5-abf8-43be-a1c9-9555b29c931e",
   "metadata": {},
   "source": [
    "## Preview your subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc006dbb-2564-4afb-9d20-e17b2c2aa8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some observations have multiple dithers\n",
    "ncols = 3\n",
    "nrows = len(bgnd_sub_imgs)\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(6*ncols, 6*nrows))\n",
    "\n",
    "for i, (targ_file, pair) in enumerate(bgnd_img_mapper.items()):\n",
    "    obsnum = targ_file[7:10]\n",
    "\n",
    "    # pull the images\n",
    "    targ_img = pair['targ']\n",
    "    bgnd_img = pair['bgnd']\n",
    "    bgsub_img = bgnd_sub_imgs[targ_file]\n",
    "    # reduce dimensions for plotting\n",
    "    while np.ndim(targ_img) > 2:\n",
    "        targ_img = np.nanmedian(targ_img, axis=0)\n",
    "    while np.ndim(bgnd_img) > 2:\n",
    "        bgnd_img = np.nanmedian(bgnd_img, axis=0)\n",
    "    while np.ndim(bgsub_img) > 2:\n",
    "        bgsub_img = np.nanmedian(bgsub_img, axis=0)\n",
    "\n",
    "    for img, ax in zip((targ_img, bgnd_img, bgsub_img), axes[i]):\n",
    "        vmin, vmax = np.quantile(img, [0.01, 0.99])\n",
    "        ax.imshow(img, vmin=vmin, vmax=vmax, origin='lower', cmap=mpl.cm.magma)\n",
    "        ax.set_aspect('equal')\n",
    "\n",
    "    axes[i, 0].set_ylabel(f\"Obs {obsnum}\", size='x-large')\n",
    "    axes[i, 1].set_ylabel(f\"Obs {dict(bgnd_obsnum_mapper)[obsnum]}\", size='x-large')\n",
    "\n",
    "axes[0][0].set_title(\"Target\", size='x-large')\n",
    "axes[0][1].set_title(\"Background\", size='x-large');\n",
    "axes[0][2].set_title(\"Background Removed\", size='x-large');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdc9e7a-d85b-4390-9790-37c670a2e20b",
   "metadata": {},
   "source": [
    "## Write images to file\n",
    "\n",
    "You can either replace the SCI HDU data, or write a new fits file. It's important to remember to preserve the ASDF extension because that contains the WCS information used by the pipeline for alignment. In this example, we are going to write new files with `bgsub` appended to the end fo the filename.\n",
    "\n",
    "The target folder is `'test/bgnd_sub_imgs/'` but you can set it to anything you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdac0615-bc26-4037-a884-69d66e38e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "bgsub_path = Path(\"test/bgnd_sub_imgs/\")\n",
    "\n",
    "def write_bgsub(targ_file, img, path):\n",
    "    \"\"\"Write the background-subtracted version of a file to disk\"\"\"\n",
    "    targ_file = Path(targ_file)\n",
    "    bgsub_name = Path(path) / (targ_file.stem + '_bgsub' + targ_file.suffix)\n",
    "    with fits.open(targ_file) as hdulist:\n",
    "        hdus = [i.copy() for i in hdulist]\n",
    "    hdus[1].data = img\n",
    "    bgsub_hdulist = fits.HDUList(hdus)\n",
    "    bgsub_hdulist.writeto(bgsub_name, overwrite='True')\n",
    "    print(\"Wrote\", bgsub_name)\n",
    "for targ_file, img in bgnd_sub_imgs.items():\n",
    "    write_bgsub(data_folder / targ_file, img, bgsub_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa3d76f-3331-4643-ac59-11278d15bc3e",
   "metadata": {},
   "source": [
    "# Level 3 pipeline\n",
    "\n",
    "For running the Level 3 pipeline, you have need an association file to tell the pipeline which exposures belong to the reference star and which to the science star. There are two ways to do this:\n",
    "1) If you downloaded the association files (ASN, in MAST), find the association file you want and replace the filenames with the filenames of the background-subtracted version.\n",
    "2) Write your own dummy file, which is what we will do here.\n",
    "\n",
    "We will perform this step once for each of the coronagraph subarrays: 1065, 1140, and 1550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159ed391-b5b3-44c2-9f08-67044fdd778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline output directory\n",
    "output_parent = Path(\"./test\")\n",
    "pipeline_output = output_parent / \"pipeline_output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df9c229-236e-4215-948b-bed9ce22cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dummy_asn(filename, name, filedict):\n",
    "    \"\"\"\n",
    "    Write a dummy ASN file for manually processing files through Level 3\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filedict: dict\n",
    "      dict where the key is the relative path and filename for the association\n",
    "      file, and the value is \"science\" or \"psf\"\n",
    "    name: prefix for the stage 3 output files\n",
    "    sci_files: list of str\n",
    "      list of paths to the science image files\n",
    "    psf_files: list of str\n",
    "      list of paths to the psf image files\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    association file written to given location\n",
    "\n",
    "    \"\"\"\n",
    "    # make the specifications for the image files\n",
    "    def make_entries(filedict, ntabs=3):\n",
    "        tab='\\t'\n",
    "        line = lambda key, val: f\"{tab*(ntabs+1)}\\\"expname\\\": \\\"{key}\\\",\\n{tab*(ntabs+1)}\\\"exptype\\\": \\\"{val}\\\"\"\n",
    "        lines = f\"\\n{tab*ntabs}}},{{\\n\".join(line(key, val) for key, val in filedict.items())\n",
    "        return lines\n",
    "    file_str = make_entries(filedict, 5)\n",
    "\n",
    "    template = f\"\"\"{{\n",
    "    \"asn_type\": \"coron3\",\n",
    "    \"asn_rule\": \"candidate_Asn_Lv3Coron\",\n",
    "    \"program\": \"{name}\",\n",
    "    \"asn_id\": \"c1001\",\n",
    "    \"target\": \"dummy\",\n",
    "    \"asn_pool\": \"{name}-pool\",\n",
    "    \"products\": [{{\n",
    "        \"name\": \"{name}\",\n",
    "        \"members\": [{{\n",
    "    {file_str}\n",
    "    }}]\n",
    "    }}]\n",
    "    }}\"\"\"\n",
    "    # make sure the file ends in .json\n",
    "    filename = Path(filename).with_suffix(\".json\")\n",
    "    with open(str(filename), 'w') as ff:\n",
    "        ff.write(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141e823e-17c4-4d7b-905a-68a47260df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(bgsub_path.glob(\"jw*calints_bgsub.fits\"))\n",
    "obsnum_files = {f.name: f.name[7:10] for f in files}\n",
    "obsnum_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa74d6a-e7dd-4901-a8de-dbb4e86c954a",
   "metadata": {},
   "source": [
    "## 1065\n",
    "\n",
    "use the APT file to figure out which observation numbers correspond to the science target and which to the reference PSF target.\n",
    "For HIP 65426's F1140C observations, Observations 4 and 5 are the science, and 6 are the references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f909a3-5f16-4aea-9f41-2b73e08f165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_obsnums = ['019', '020']\n",
    "ref_obsnum = '021'\n",
    "sci = {f: obsnum for f, obsnum in obsnum_files.items() if obsnum in sci_obsnums}\n",
    "ref = {f: obsnum for f, obsnum in obsnum_files.items() if obsnum == ref_obsnum}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2d035f-80c4-4aa2-bbad-33f0f3e4a958",
   "metadata": {},
   "outputs": [],
   "source": [
    "sci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6419fc6-ce28-44b5-a4f8-4d6d5b60682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b736e06d-0c7c-4e9b-964a-9cdc965b8908",
   "metadata": {},
   "source": [
    "Copy the above filenames into a new association file and write it to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ed3eb6-611f-4ced-8946-44bc6ea585e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "asn_file = output_parent / \"stage3_asn_hd141569a_1065.json\"\n",
    "\n",
    "# list the files and whether they are science or psf\n",
    "files = {}\n",
    "for f in sci.keys():\n",
    "    files[str(bgsub_path.absolute() / f)] = 'science'\n",
    "for f in ref.keys():\n",
    "    files[str(bgsub_path.absolute() / f)] = 'psf'\n",
    "    \n",
    "write_dummy_asn(asn_file, \"01386_hd141569a_1065\", files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1853e474-cd0a-4eec-8d70-424ef00d0cfc",
   "metadata": {},
   "source": [
    "Run Stage 3 with a hand-made association file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ad235b-2a80-4cba-82e9-5d72ef4d05e8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# it's a disk target so we don't want to use many KLIP modes - set it to 1. Default is 50\n",
    "params = {'output_dir': str(pipeline_output.absolute()), # default is '.'\n",
    "          'steps': { # this is optional\n",
    "              'klip': {'truncate': 5}\n",
    "                   }\n",
    "         }\n",
    "cor3 = Coron3Pipeline.call(str(asn_file), **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefd069f-36d3-4083-a5bf-27b5506cbaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = fits.getdata(str(pipeline_output / '01386_hd141569a_1065_i2d.fits'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10783ba2-1401-4735-af4e-36bb83b46fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8,8))\n",
    "vmin, vmax = np.quantile(img, [0.01, 0.99])\n",
    "ax.imshow(img, origin='lower', vmin=vmin, vmax=vmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d7fe88-26f8-46b5-8da4-6bfdf4a3f4e4",
   "metadata": {},
   "source": [
    "## 1140\n",
    "For F1140C, Observations 22 and 23 are the science, and 24 are the references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e688869-9f45-464f-884c-7c4c83e35ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_obsnums = ['022','023']\n",
    "ref_obsnum = '024'\n",
    "sci = {f: obsnum for f, obsnum in obsnum_files.items() if obsnum in sci_obsnums}\n",
    "ref = {f: obsnum for f, obsnum in obsnum_files.items() if obsnum == ref_obsnum}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd333ec-62bc-495a-8966-588882023939",
   "metadata": {},
   "outputs": [],
   "source": [
    "sci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c43e22-6a20-46ab-8142-c482ce73d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3207d814-492b-4b2c-b949-001f991c669e",
   "metadata": {},
   "source": [
    "Copy the above filenames into a new association file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88637d6a-9c5f-42c5-a773-f76df959df6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the files and whether they are science or psf\n",
    "files = {}\n",
    "for f in sci.keys():\n",
    "    files[str(bgsub_path.absolute() / f)] = 'science'\n",
    "for f in ref.keys():\n",
    "    files[str(bgsub_path.absolute() / f)] = 'psf'\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2966d829-513a-4675-91f1-9c7c1dbaea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the asn file\n",
    "asn_file = output_parent / \"stage3_asn_hd141569a_1140.json\"\n",
    "\n",
    "write_dummy_asn(asn_file, \"01386_hd141569a_1140\", files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff487d04-db89-4357-8270-569e347ba63b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# it's a disk target so we don't want to use many KLIP modes - set it to 1. Default is 50\n",
    "params = {'output_dir': str(pipeline_output.absolute()), # default is '.'\n",
    "          'steps': { # this is optional\n",
    "              'klip': {'truncate': 5}\n",
    "                   }\n",
    "         }\n",
    "cor3 = Coron3Pipeline.call(str(asn_file), **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2ff754-f0ad-416c-803f-21e2bf209cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = fits.getdata(str(pipeline_output / \"01386_hd141569a_1140_i2d.fits\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5358e75e-5859-46b9-8c3a-54b736a428f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8,8))\n",
    "vmin, vmax = np.quantile(img, [0.01, 0.99])\n",
    "ax.imshow(img, origin='lower', vmin=vmin, vmax=vmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a447c38d-2f1a-4f32-b51c-f9beeaf2a62c",
   "metadata": {},
   "source": [
    "# 1550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2950b364-2849-4a0e-8261-e0c5130c71c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_obsnums = ['025', '026']\n",
    "ref_obsnum = '027'\n",
    "sci = {f: obsnum for f, obsnum in obsnum_files.items() if obsnum in sci_obsnums}\n",
    "ref = {f: obsnum for f, obsnum in obsnum_files.items() if obsnum == ref_obsnum}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e74a0c-f688-4437-9c05-1b501fbeb447",
   "metadata": {},
   "outputs": [],
   "source": [
    "sci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9113430-0053-4407-8b03-4ab0c32f879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d53a65d-0be6-493d-978d-8012fc3382e6",
   "metadata": {},
   "source": [
    "Copy the above filenames into a new association file and write it to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2e9972-20b4-4956-887e-2218e0f44c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "asn_file = output_parent / \"stage3_asn_hd141569a_1550.json\"\n",
    "\n",
    "# list the files and whether they are science or psf\n",
    "files = {}\n",
    "for f in sci.keys():\n",
    "    files[str(bgsub_path.absolute() / f)] = 'science'\n",
    "for f in ref.keys():\n",
    "    files[str(bgsub_path.absolute() / f)] = 'psf'\n",
    "    \n",
    "write_dummy_asn(asn_file, \"01386_hd141569a_1550\", files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4021cd-f198-460f-b963-e37d350f6d5c",
   "metadata": {},
   "source": [
    "Run Stage 3 with a hand-made association file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10db84c2-73c2-417a-8a4a-3ffe8d89c165",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# it's a disk target so we don't want to use many KLIP modes - set it to 1. Default is 50\n",
    "params = {'output_dir': str(pipeline_output.absolute()), # default is '.'\n",
    "          'steps': { # this is optional\n",
    "              'klip': {'truncate': 5}\n",
    "                   }\n",
    "         }\n",
    "cor3 = Coron3Pipeline.call(str(asn_file), **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154b47a5-538e-4453-920d-fb1d27a25617",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = fits.getdata(str(pipeline_output / '01386_hd141569a_1550_i2d.fits'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dcb2d9-6c56-4509-bf3e-632b7a3fa206",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8,8))\n",
    "vmin, vmax = np.quantile(img, [0.01, 0.99])\n",
    "ax.imshow(img, origin='lower', vmin=vmin, vmax=vmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a364a8bc-c9c5-42ac-a39e-2c503f585238",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jwst_inspector]",
   "language": "python",
   "name": "conda-env-jwst_inspector-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
